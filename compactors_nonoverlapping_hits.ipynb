{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb0c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b0845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load compactors into a list. \n",
    "sponge_cr = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/sponge_compactors_with_repeat_calls.tsv',sep='\\t')\n",
    "sponge_cr = sponge_cr[~sponge_cr['dataset'].str.contains('all_samples')]\n",
    "arrr = sponge_cr['compactor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write compactors to a FASTA. \n",
    "os.mkdir('sponge_pfam')\n",
    "file = open('sponge_pfam/compactors.fasta','a')\n",
    "for i in range(len(arrr)):\n",
    "    file.write('>'+str(i)+'\\n'+arrr[i]+'\\n')\n",
    "file.close()\n",
    "os.chdir('sponge_pfam')\n",
    "\n",
    "#### Run the Pfam script on the compactors. \n",
    "os.system('sbatch /oak/stanford/groups/horence/george/splash_utils/pfam_multithread.sh compactors.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c53ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load files whose anchors we have compactors for.\n",
    "## Define fields we want. \n",
    "usec=['anchor','effect_size_bin',\n",
    " 'number_nonzero_samples',\n",
    " 'target_entropy',\n",
    " 'avg_edit_distance_max_target']\n",
    "sout = glob.glob('/oak/stanford/groups/horence/Roozbeh/Sponge_project/runs/10X_runs/Sponge_SRP216435/*/result.after_correction.scores.tsv')\n",
    "fir = pd.read_csv(sout[0],usecols=usec,sep='\\t')\n",
    "fir['dataset'] = sout[0]\n",
    "for i in range(1,len(sout)):\n",
    "    nex = pd.read_csv(sout[i],sep='\\t',usecols=usec)\n",
    "    nex['dataset'] = sout[i]\n",
    "    fir = pd.concat([fir,nex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b78d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_stats = sponge_cr.merge(fir,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9aba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_files = glob.glob('sponge_pfam/*fasta')\n",
    "\n",
    "pfams = pd.read_csv(pfam_files[0],engine='python',sep='\\t',header=None)\n",
    "for i in range(1,len(pfam_files)):\n",
    "    try:\n",
    "        pfams_1 = pd.read_csv(pfam_files[i],engine='python',sep='\\t',header=None)\n",
    "        pfams = pd.concat([pfams_1,pfams])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        pass\n",
    "\n",
    "pfams = pd.DataFrame({'header':[i for i in pfams[0] if i[0]=='>'],'sequence':[i for i in pfams[0] if i[0]!='>']})\n",
    "\n",
    "pfams['header'] = [i[1:] for i in pfams['header'] ]\n",
    "\n",
    "\n",
    "pfam_files = glob.glob('sponge_pfam/*PFAM.tblout')\n",
    "\n",
    "pfams1 = pd.read_csv(pfam_files[0],engine='python',sep='\\t',header=None)\n",
    "for i in range(1,len(pfam_files)):\n",
    "    try:\n",
    "        pfams_1 = pd.read_csv(pfam_files[i],engine='python',sep='\\t',header=None)\n",
    "        pfams1 = pd.concat([pfams_1,pfams1])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        pass\n",
    "    \n",
    "## Define the fields in the Pfam output. \n",
    "fields = ['header','accession','tlen','query_name','accession2','qlen','full_seq_evalue','full_seq_score','full_seq_bias','this_domain_number','this_domain_of','this_domain_c_evalue','this_domain_i_evalue','this_domain_score','this_domain_bias','hmm_coord_from','hmm_coord_to','ali_coord_from','ali_coord_to','env_coord_from','env_coord_to','acc','description_of_target']\n",
    "\n",
    "## Parse the entries in the Pfam table which are not related to formatting. \n",
    "pfam_list_compactor = [i for i in list(pfams1.iloc[:,0]) if '#' not in i]\n",
    "compactor_ok = [ i.split(' ') for i in pfam_list_compactor]\n",
    "compactor_okok = []\n",
    "for i in compactor_ok: \n",
    "    lis = [j for j in i if j]\n",
    "    lis = lis[:22] + [' '.join(lis[23:])]\n",
    "    compactor_okok.append(lis)\n",
    "\n",
    "compactor_pfam_structured = pd.DataFrame(compactor_okok, columns = fields)\n",
    "compactor_pfam_structured = compactor_pfam_structured.add_prefix('Pfam_') \n",
    "\n",
    "## As the table has been loaded as a string, convert the numerical field (e-value) to a float to ensure \n",
    "## integrity of operations using this value. \n",
    "compactor_pfam_structured['Pfam_full_seq_evalue'] = compactor_pfam_structured['Pfam_full_seq_evalue'].astype(float)\n",
    "\n",
    "## Filter so that we retain e-values < 0.05. \n",
    "compactor_pfam_structured = compactor_pfam_structured[compactor_pfam_structured['Pfam_full_seq_evalue']<0.05].reset_index(drop=True)\n",
    "\n",
    "compactor_pfam_structured = compactor_pfam_structured.rename(columns={'Pfam_header':'header'})\n",
    "\n",
    "compactor_pfam_structured['Pfam_frame'] = [int(i.split('=')[1]) for i in compactor_pfam_structured['header']]\n",
    "compactor_pfam_structured['Pfam_strand'] = [i>0 for i in compactor_pfam_structured['Pfam_frame']]\n",
    "\n",
    "compactor_pfam_structured[['Pfam_env_coord_from','Pfam_env_coord_to']] = compactor_pfam_structured[['Pfam_env_coord_from','Pfam_env_coord_to']].astype(int)\n",
    "compactor_pfam_structured[['Pfam_this_domain_i_evalue']] = compactor_pfam_structured[['Pfam_this_domain_i_evalue']].astype(float)\n",
    "compactor_pfam_structured['Pfam_strand'] = compactor_pfam_structured['Pfam_strand'].replace({True:'+',False:'-'})\n",
    "\n",
    "## Get a Pfam results file. \n",
    "compactor_pfam_structured['header'] = compactor_pfam_structured['header'].str.split('_').str[0]\n",
    "pfam_hits = compactor_pfam_structured.merge(pfams,how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06a317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inds(data):\n",
    "    \n",
    "    ## Create a local copy. \n",
    "    copy = data.sort_values('Pfam_this_domain_i_evalue')\n",
    "\n",
    "    ## Get the index corresponding to the best e-value hit. \n",
    "    take_index = [int(copy.index[0])]\n",
    "    \n",
    "    ## Get the range of seqeunce coordinates corresponding to the hit. \n",
    "    initial_coord_set = set(list(range(int(copy['Pfam_env_coord_from'][take_index[0]]),int(copy['Pfam_env_coord_to'][take_index[0]]+1))))\n",
    "    \n",
    "    ## For each remaining hit: \n",
    "    for ind in copy.index[1:]:\n",
    "        \n",
    "        ## Extrac the range of sequence coordinates. \n",
    "        this_coord_set = set(list(range(int(copy['Pfam_env_coord_from'][ind]),int(copy['Pfam_env_coord_to'][ind])+1)))\n",
    "        \n",
    "        ## Get the intersect of this hit's sequence coordinates and all accepted hits' coordinates. \n",
    "        intersect = set(this_coord_set&initial_coord_set)\n",
    "        \n",
    "        ## If the intersect size is 0, we do not have a best hit for these coordinates, so we add this one.\n",
    "        ## Further, we update the set of coordinates for which we have best hits. \n",
    "        if len(intersect) == 0:\n",
    "            take_index.append(ind)\n",
    "            initial_coord_set = set(this_coord_set|initial_coord_set)\n",
    "\n",
    "    ## We return the indices corresponding to best hits covering the input sequence. \n",
    "    return take_index\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(int(os.environ['SLURM_JOB_CPUS_PER_NODE'])) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    return ret_list\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    \"\"\"\n",
    "    https://realpython.com/python-flatten-list/\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68458971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get nonoverlapping hits. \n",
    "gpby = pfam_hits.groupby(['header','Pfam_strand'])[['Pfam_this_domain_i_evalue','Pfam_env_coord_from','Pfam_env_coord_to']]\n",
    "outs = applyParallel(gpby, inds)\n",
    "best_spots = pfam_hits.loc[flatten_extend(outs)]\n",
    "best_spots = best_spots.rename(columns={'sequence':'compactor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5491a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A file reporting compactors, their SPLASH statistics, all (nonoverlapping) Pfam hits. \n",
    "stats_and_all_hits = with_stats.merge(best_spots,how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
