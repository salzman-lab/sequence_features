{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc38cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9ed9a",
   "metadata": {},
   "source": [
    "#### First, load all compactors that were submitted to Pfam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcb54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_files = glob.glob('pfam_with_position/Pfam_alignment/*fasta')\n",
    "\n",
    "pfams = pd.read_csv(pfam_files[0],engine='python',sep='\\t',header=None)\n",
    "for i in range(1,len(pfam_files)):\n",
    "    try:\n",
    "        pfams_1 = pd.read_csv(pfam_files[i],engine='python',sep='\\t',header=None)\n",
    "        pfams = pd.concat([pfams_1,pfams])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        pass\n",
    "\n",
    "pfams = pd.DataFrame({'header':[i for i in pfams[0] if i[0]=='>'],'sequence':[i for i in pfams[0] if i[0]!='>']})\n",
    "\n",
    "pfams['header'] = [i[1:] for i in pfams['header'] ]\n",
    "\n",
    "pfams.to_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/compactors_input_to_Pfam.tsv',sep='\\t',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05117f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfams = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/compactors_input_to_Pfam.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f8330",
   "metadata": {},
   "source": [
    "#### Now, load all relevant SPLASH statistics for these anchors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9511a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/mining_10x_ss2_Pfam_compactors_SPLASH_stats.tsv',sep='\\t',usecols=['anchor','ds','effect_size_bin','number_nonzero_samples','target_entropy','avg_edit_distance_max_target'])\n",
    "stats2 = pd.read_csv('mining_botryllus_ss2_Pfam_compactors_SPLASH_stats.tsv',sep='\\t',usecols=['anchor','ds','effect_size_bin','number_nonzero_samples','target_entropy','avg_edit_distance_max_target'])\n",
    "stats = pd.concat([stats2.drop_duplicates(),stats.drop_duplicates()]).reset_index(drop=True)\n",
    "\n",
    "botryllus = pd.read_csv('/oak/stanford/groups/horence/george/botryllus_december_2023/all_compactors_and_SPLASH_statistics_and_Pfam.tsv',sep='\\t',usecols=['compactor','exact_support','ds'])\n",
    "m_test = pd.read_csv('/oak/stanford/groups/horence/george/mass_pfam_01192024/mining_10x_ss2_mkokot.tsv',sep='\\t',usecols=['compactor','exact_support','ds'])\n",
    "\n",
    "botryllus = botryllus.drop_duplicates()\n",
    "m_test = m_test.drop_duplicates()\n",
    "compactor_support_ds = pd.concat([m_test,botryllus]).reset_index(drop=True)\n",
    "compactor_support_ds['anchor'] = [i[:27] for i in compactor_support_ds['compactor']]\n",
    "\n",
    "compactors_stats = compactor_support_ds.merge(stats,how='left')\n",
    "compactors_stats.to_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/compactors_input_to_Pfam_with_SPLASH_stats.tsv',sep='\\t',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38c7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "compactors_stats = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/compactors_input_to_Pfam_with_SPLASH_stats.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e38b7",
   "metadata": {},
   "source": [
    "#### Join the SPLASH statistics left onto the compactors with headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afe3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfams['anchor'] = [i[:27] for i in pfams['sequence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3460b56",
   "metadata": {},
   "source": [
    "#### Load compactor Pfam results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_files = glob.glob('pfam_with_position/Pfam_alignment/*PFAM.tblout')\n",
    "\n",
    "pfams1 = pd.read_csv(pfam_files[0],engine='python',sep='\\t',header=None)\n",
    "for i in range(1,len(pfam_files)):\n",
    "    try:\n",
    "        pfams_1 = pd.read_csv(pfam_files[i],engine='python',sep='\\t',header=None)\n",
    "        pfams1 = pd.concat([pfams_1,pfams1])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        pass\n",
    "    \n",
    "## Define the fields in the Pfam output. \n",
    "fields = ['header','accession','tlen','query_name','accession2','qlen','full_seq_evalue','full_seq_score','full_seq_bias','this_domain_number','this_domain_of','this_domain_c_evalue','this_domain_i_evalue','this_domain_score','this_domain_bias','hmm_coord_from','hmm_coord_to','ali_coord_from','ali_coord_to','env_coord_from','env_coord_to','acc','description_of_target']\n",
    "\n",
    "## Parse the entries in the Pfam table which are not related to formatting. \n",
    "pfam_list_compactor = [i for i in list(pfams1.iloc[:,0]) if '#' not in i]\n",
    "compactor_ok = [ i.split(' ') for i in pfam_list_compactor]\n",
    "compactor_okok = []\n",
    "for i in compactor_ok: \n",
    "    lis = [j for j in i if j]\n",
    "    lis = lis[:22] + [' '.join(lis[23:])]\n",
    "    compactor_okok.append(lis)\n",
    "\n",
    "compactor_pfam_structured = pd.DataFrame(compactor_okok, columns = fields)\n",
    "compactor_pfam_structured = compactor_pfam_structured.add_prefix('Pfam_') \n",
    "\n",
    "## As the table has been loaded as a string, convert the numerical field (e-value) to a float to ensure \n",
    "## integrity of operations using this value. \n",
    "compactor_pfam_structured['Pfam_full_seq_evalue'] = compactor_pfam_structured['Pfam_full_seq_evalue'].astype(float)\n",
    "\n",
    "## Filter so that we retain e-values < 0.05. \n",
    "compactor_pfam_structured = compactor_pfam_structured[compactor_pfam_structured['Pfam_full_seq_evalue']<0.05].reset_index(drop=True)\n",
    "\n",
    "compactor_pfam_structured = compactor_pfam_structured.rename(columns={'Pfam_header':'header'})\n",
    "\n",
    "compactor_pfam_structured = compactor_pfam_structured.merge(pfams,how='left')\n",
    "compactor_pfam_structured['Pfam_frame'] = [int(i.split('=')[1]) for i in compactor_pfam_structured['header']]\n",
    "compactor_pfam_structured['Pfam_strand'] = [i>0 for i in compactor_pfam_structured['Pfam_frame']]\n",
    "\n",
    "compactor_pfam_structured[['Pfam_env_coord_from','Pfam_env_coord_to']] = compactor_pfam_structured[['Pfam_env_coord_from','Pfam_env_coord_to']].astype(int)\n",
    "compactor_pfam_structured[['Pfam_this_domain_i_evalue']] = compactor_pfam_structured[['Pfam_this_domain_i_evalue']].astype(float)\n",
    "compactor_pfam_structured['Pfam_strand'] = compactor_pfam_structured['Pfam_strand'].replace({True:'+',False:'-'})\n",
    "\n",
    "## Write the Pfam results file. \n",
    "\n",
    "compactor_pfam_structured.to_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/Pfam_hits.tsv',sep='\\t',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bec5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compactor_pfam_structured = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/Pfam_hits.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce5f70",
   "metadata": {},
   "source": [
    "#### Use lookup table to perform contaminant filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"file = open('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/lookup_input.fasta','a')\n",
    "\n",
    "for i in pfams.index:\n",
    "    \n",
    "    file.write('>'+pfams['header'][i]+'\\n'+pfams['compactor'][i]+'\\n')\n",
    "    \n",
    "file.close()\n",
    "\n",
    "os.system('sbatch /oak/stanford/groups/horence/george/splash_utils/lookup_242_fasta.sh /oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/lookup_input.fasta /oak/stanford/groups/horence/george/lookup_table_index_02082024/mining_built_index /oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/lookup_out.tsv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'grep -vn \"Carp_GCA_019924925\\|Ralstonia\\|SRA_synthetic_barcodes\\|illumina_adapters\\|GCA_004000535\\|UniVec\" lookup_out.tsv | cut -f1 -d: > internal_files/non_contaminant_indices.txt '\n",
    "command2 = 'grep -n \"final_purged_primary\\|final_purged_haplotigs\\|botznik-chr\" lookup_out.tsv | cut -f1 -d: > internal_files/botryllus_indices.txt'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "857646e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the compactors that do not show up in any contaminant file. \n",
    "\n",
    "indic = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/internal_files/non_contaminant_indices.txt',header=None)\n",
    "indic[0] = [i-1 for i in indic[0]]\n",
    "\n",
    "acceptable_compactors = pfams.loc[indic[0]][['compactor']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df094c03",
   "metadata": {},
   "source": [
    "#### Reduce this set of compactor hits to that set representing nonoverlapping best hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feaed1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inds(data):\n",
    "    \n",
    "    ## Create a local copy. \n",
    "    copy = data.sort_values('Pfam_this_domain_i_evalue')\n",
    "\n",
    "    ## Get the index corresponding to the best e-value hit. \n",
    "    take_index = [int(copy.index[0])]\n",
    "    \n",
    "    ## Get the range of seqeunce coordinates corresponding to the hit. \n",
    "    initial_coord_set = set(list(range(int(copy['Pfam_env_coord_from'][take_index[0]]),int(copy['Pfam_env_coord_to'][take_index[0]]+1))))\n",
    "    \n",
    "    ## For each remaining hit: \n",
    "    for ind in copy.index[1:]:\n",
    "        \n",
    "        ## Extrac the range of sequence coordinates. \n",
    "        this_coord_set = set(list(range(int(copy['Pfam_env_coord_from'][ind]),int(copy['Pfam_env_coord_to'][ind])+1)))\n",
    "        \n",
    "        ## Get the intersect of this hit's sequence coordinates and all accepted hits' coordinates. \n",
    "        intersect = set(this_coord_set&initial_coord_set)\n",
    "        \n",
    "        ## If the intersect size is 0, we do not have a best hit for these coordinates, so we add this one.\n",
    "        ## Further, we update the set of coordinates for which we have best hits. \n",
    "        if len(intersect) == 0:\n",
    "            take_index.append(ind)\n",
    "            initial_coord_set = set(this_coord_set|initial_coord_set)\n",
    "\n",
    "    ## We return the indices corresponding to best hits covering the input sequence. \n",
    "    return take_index\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(int(os.environ['SLURM_JOB_CPUS_PER_NODE'])) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    return ret_list\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    \"\"\"\n",
    "    https://realpython.com/python-flatten-list/\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca12836",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpby = compactor_pfam_structured.groupby(['header','Pfam_strand'])[['Pfam_this_domain_i_evalue','Pfam_env_coord_from','Pfam_env_coord_to']]\n",
    "outs = applyParallel(gpby, inds)\n",
    "best_spots = compactor_pfam_structured.loc[flatten_extend(outs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3eef7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5857829, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfams = pfams.rename(columns={'sequence':'compactor'})\n",
    "compactors_stats = compactors_stats.merge(pfams,how='left')\n",
    "assert compactors_stats.shape == (5857829, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4253024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge these selected Pfam hits with the anchors and SPLASH statistics. \n",
    "best_spots = best_spots[[i for i in best_spots.columns if i not in ['sequence','anchor']]]\n",
    "best_spots['Pfam_header'] = best_spots['header']\n",
    "best_spots['header'] = [i.split('_frame')[0] for i in best_spots['Pfam_header']]\n",
    "pfam_with_stats = compactors_stats.merge(best_spots)\n",
    "\n",
    "#### Introduce length. \n",
    "pfam_with_stats['length'] = [len(i) for i in pfam_with_stats['compactor']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802f8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_with_stats = pd.read_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/best_nonoverlapping_Pfam_hits.tsv',sep='\\t')\n",
    "marek_metadata = pd.read_csv('/scratch/groups/horence/mkokot/2023-11-16/SraRunTable.csv',sep=',')[['Experiment','Organism']]\n",
    "pfam_with_stats['Experiment'] = [i.split('/')[-1] for i in pfam_with_stats['ds']]\n",
    "metadata = pd.concat([marek_metadata, pd.DataFrame({'Experiment':['cell_island_brain','brain'],'Organism':['Botryllus schlosseri','Botryllus schlosseri']})]).drop_duplicates().reset_index(drop=True)\n",
    "pfam_with_stats.merge(metadata,how='left').to_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/best_nonoverlapping_Pfam_hits.tsv',sep='\\t',index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfe043",
   "metadata": {},
   "source": [
    "#### Define a function to check for co-occurrence of domains. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80dab0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_table(data, length):\n",
    "    \n",
    "    ## Take only sequences of a particular length.\n",
    "    subset = data[data['length'] == length].dropna(subset='Pfam_query_name')\n",
    "    \n",
    "    ## Get the number of unique compactors per anchor, Pfam domain pair. \n",
    "    test = subset.groupby(['anchor','Pfam_query_name'])['compactor'].nunique().reset_index()\n",
    "    \n",
    "    ## Get compactor's unique count and get the domains in the top 3 by # assigned compactors for the anchor. \n",
    "    test['domain_rank'] = test.groupby(['anchor'])['compactor'].rank('first',ascending=False)\n",
    "    test = test.rename(columns={'compactor':'compactor_count'})\n",
    "    test = test[test['domain_rank'] < 4]\n",
    "    test['domain_rank'] = test['domain_rank'].astype(int)\n",
    "    top_three = subset[['anchor','compactor','Pfam_query_name']].drop_duplicates().reset_index(drop=True)\\\n",
    "    .merge(test[['anchor','Pfam_query_name','domain_rank','compactor_count']])\n",
    "\n",
    "    ## Get counts for compactors having domains 1 and 2. \n",
    "    one_two = top_three[top_three['domain_rank'].isin([1,2])]\n",
    "    one_two = one_two.groupby(['anchor','compactor'])['Pfam_query_name'].nunique().reset_index()\n",
    "    one_two = one_two[one_two['Pfam_query_name']>1]\n",
    "    one_two = one_two.groupby('anchor')['compactor'].nunique().reset_index().rename(columns={'compactor':'1_2'})\n",
    "    \n",
    "    ## Get counts for compactors having domains 1 and 3. \n",
    "    one_three = top_three[top_three['domain_rank'].isin([1,3])]\n",
    "    one_three = one_three.groupby(['anchor','compactor'])['Pfam_query_name'].nunique().reset_index()\n",
    "    one_three = one_three[one_three['Pfam_query_name']>1]\n",
    "    one_three = one_three.groupby('anchor')['compactor'].nunique().reset_index().rename(columns={'compactor':'1_3'})\n",
    "    \n",
    "    ## Get counts for compactors having domains 3 and 2. \n",
    "    two_three = top_three[top_three['domain_rank'].isin([3,2])]\n",
    "    two_three = two_three.groupby(['anchor','compactor'])['Pfam_query_name'].nunique().reset_index()\n",
    "    two_three = two_three[two_three['Pfam_query_name']>1]\n",
    "    two_three = two_three.groupby('anchor')['compactor'].nunique().reset_index().rename(columns={'compactor':'2_3'})\n",
    "\n",
    "    ## Get counts for compactors having domains 1, 2, and 3. \n",
    "    one_two_three = top_three.copy()\n",
    "    one_two_three = one_two_three.groupby(['anchor','compactor'])['Pfam_query_name'].nunique().reset_index()\n",
    "    one_two_three = one_two_three[one_two_three['Pfam_query_name']>2]\n",
    "    one_two_three = one_two_three.groupby('anchor')['compactor'].nunique().reset_index().rename(columns={'compactor':'1_2_3'})\n",
    "\n",
    "    ## Get counts (stratified by intersect) into one table. \n",
    "    summarize_intersect = one_two.merge(one_three,how='outer').fillna(0)\\\n",
    "    .merge(two_three,how='outer').fillna(0)\\\n",
    "    .merge(one_two_three,how='outer').fillna(0)\n",
    "\n",
    "    top_three = top_three[['anchor','Pfam_query_name','domain_rank','compactor_count']].drop_duplicates()\n",
    "    top_three['domain_rank'] = ['Domain_'+str(i) for i in top_three['domain_rank']]\n",
    "    name_top_3 = top_three.pivot(columns='domain_rank',values='Pfam_query_name',index='anchor').reset_index()\n",
    "    top_three['domain_rank'] = [i + '_Count' for i in top_three['domain_rank']]\n",
    "    top_three = top_three.pivot(columns='domain_rank',values='compactor_count',index='anchor').reset_index()\n",
    "\n",
    "    domains_counts_per_anchor = name_top_3.merge(top_three)\n",
    "    if 'Domain_2_Count' not in domains_counts_per_anchor.columns: \n",
    "        domains_counts_per_anchor['Domain_2_Count'], domains_counts_per_anchor['Domain_3_Count'] = [0 for i in range(domains_counts_per_anchor.shape[0])],[0 for i in range(domains_counts_per_anchor.shape[0])]\n",
    "    elif 'Domain_3_Count' not in domains_counts_per_anchor.columns:\n",
    "        domains_counts_per_anchor['Domain_3_Count'] = [0 for i in range(domains_counts_per_anchor.shape[0])]\n",
    "    domains_counts_per_anchor['Domain_1_Count'], domains_counts_per_anchor['Domain_2_Count'], domains_counts_per_anchor['Domain_3_Count'] = domains_counts_per_anchor['Domain_1_Count'].fillna(0).astype(int), domains_counts_per_anchor['Domain_2_Count'].fillna(0).astype(int), domains_counts_per_anchor['Domain_3_Count'].fillna(0).astype(int)\n",
    "    domains_counts_per_anchor = domains_counts_per_anchor.merge(summarize_intersect,how='left').fillna(int(0))\n",
    "    \n",
    "    return domains_counts_per_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbc47c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_concurrence_func(data,outdir):\n",
    "    loa = dict()\n",
    "    \n",
    "    ## For each value of compactor length:\n",
    "    for i in data['length'].unique(): \n",
    "        \n",
    "        ## Make a table using the function we've defined. \n",
    "        loa[str(i)] = make_table(data,i)\n",
    "        \n",
    "    ## First assign a new column outside of the loop. \n",
    "    floa = loa[str(min(data['length'].unique()))]\n",
    "    floa['compactor_length'] = min(data['length'].unique())\n",
    "    for i in loa.keys():\n",
    "        \n",
    "        ## Assign new column if we are not revisiting the original. \n",
    "        if int(i) != min(data['length'].unique()):\n",
    "            loa[i]['compactor_length'] = int(i)\n",
    "            floa = pd.concat([floa,loa[i]])\n",
    "    \n",
    "    ## Write results across values of num_extended.\n",
    "    floa.fillna(0).to_csv(outdir+'/domain_concurrence_counts.tsv',sep='\\t',index=None)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"loa = dict()\n",
    "for i in pfam_with_stats['length'].unique(): \n",
    "    loa[str(i)] = make_table(pfam_with_stats,i)\n",
    "floa = loa['81']\n",
    "floa['compactor_length'] = 81\n",
    "for i in loa.keys():\n",
    "    if int(i) != 81:\n",
    "        loa[i]['compactor_length'] = int(i)\n",
    "        floa = pd.concat([floa,loa[i]])\n",
    "floa.fillna(0).to_csv('/oak/stanford/groups/horence/george/protein_domain_project/workflow/domain_concurrence_counts.tsv',sep='\\t',index=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5d563",
   "metadata": {},
   "source": [
    "#### Define a function to add domain-level statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f32a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stats(input_df, outpath):\n",
    "    \n",
    "    ## Given an input dataframe, deduplicate by anchor, dataset, and Pfam query name.\n",
    "    input_df1 = input_df.drop_duplicates(subset=['anchor','ds','Pfam_query_name'])\n",
    "    \n",
    "    ## Get the number of unique compactors, anchors, and samplesheets (datasets). \n",
    "    ## Get the mean, 80th and 20th percentile of effect size, entropy, # nonzero samples, and edit distance.\n",
    "    df_summary = input_df.groupby(['Pfam_query_name'])['compactor'].nunique().reset_index() \\\n",
    "    .merge(input_df.groupby(['Pfam_query_name'])['anchor'].nunique().reset_index()) \\\n",
    "    .merge(input_df.groupby(['Pfam_query_name'])['ds'].nunique().reset_index()) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['effect_size_bin'].mean().reset_index().rename(columns={'effect_size_bin':'mean_effect_size'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['effect_size_bin'].quantile(0.8).reset_index().rename(columns={'effect_size_bin':'80th_percentile_effect_size'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['effect_size_bin'].quantile(0.2).reset_index().rename(columns={'effect_size_bin':'20th_percentile_effect_size'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['target_entropy'].mean().reset_index().rename(columns={'target_entropy':'mean_entropy'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['target_entropy'].quantile(0.8).reset_index().rename(columns={'target_entropy':'80th_percentile_entropy'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['target_entropy'].quantile(0.2).reset_index().rename(columns={'target_entropy':'20th_percentile_entropy'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['number_nonzero_samples'].mean().reset_index().rename(columns={'number_nonzero_samples':'mean_nonzero_samples'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['number_nonzero_samples'].quantile(0.8).reset_index().rename(columns={'number_nonzero_samples':'80th_percentile_nonzero_samples'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['number_nonzero_samples'].quantile(0.2).reset_index().rename(columns={'number_nonzero_samples':'20th_percentile_nonzero_samples'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['avg_edit_distance_max_target'].mean().reset_index().rename(columns={'avg_edit_distance_max_target':'mean_lev_max_target'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['avg_edit_distance_max_target'].quantile(0.8).reset_index().rename(columns={'avg_edit_distance_max_target':'80th_percentile_lev_max_target'})) \\\n",
    "    .merge(input_df1.groupby(['Pfam_query_name'])['avg_edit_distance_max_target'].quantile(0.2).reset_index().rename(columns={'avg_edit_distance_max_target':'20th_percentile_lev_max_target'}))\n",
    "    \n",
    "    df_summary.to_csv(outpath,sep='\\t',index=None)\n",
    "    \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d8436d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_stats = write_stats(pfam_with_stats,'/oak/stanford/groups/horence/george/protein_domain_project/workflow/domain_summary_statistics.tsv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bda7a",
   "metadata": {},
   "source": [
    "#### Stratify these results by organism and collection type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4906761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_organism = pfam_with_stats.merge(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c993b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_technology = pfam_with_stats.copy()\n",
    "data_with_technology['technology'] = data_with_technology['ds'].str.contains('10x').replace({True:'10x',False:'SS2'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "004ae727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology')\n",
    "#os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/10x')\n",
    "#os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/SS2')\n",
    "\n",
    "write_stats(data_with_technology[data_with_technology['technology']=='SS2'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/SS2/domain_summary_statistics.tsv')\n",
    "write_stats(data_with_technology[data_with_technology['technology']=='10x'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/10x/domain_summary_statistics.tsv')\n",
    "\n",
    "write_concurrence_func(data_with_technology[data_with_technology['technology']=='SS2'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/SS2')\n",
    "write_concurrence_func(data_with_technology[data_with_technology['technology']=='10x'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology/10x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709a5174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pogona vitticeps', 'Danio rerio', 'Gallus gallus',\n",
       "       'Anolis sagrei', 'Lytechinus variegatus', 'Nicotiana tabacum',\n",
       "       'Taeniopygia guttata', 'Botryllus schlosseri',\n",
       "       'Drosophila melanogaster', 'Saccharomyces cerevisiae',\n",
       "       'Arabidopsis thaliana', 'Ciona intestinalis', 'Plasmodium vivax',\n",
       "       'Nematostella vectensis', 'Harpegnathos saltator',\n",
       "       'Plasmodium berghei', 'Eisenia andrei', 'Populus trichocarpa',\n",
       "       'Schistosoma mansoni', 'Isodiametra pulchra', 'Xenia sp.',\n",
       "       'Caenorhabditis elegans', 'Chlamydomonas reinhardtii',\n",
       "       'Plasmodium falciparum', 'Astyanax mexicanus',\n",
       "       'Nicotiana attenuata', 'Oryzias latipes', 'Zea mays',\n",
       "       'Loligo vulgaris', 'Xenopus laevis',\n",
       "       'Populus tremula x Populus alba', 'Dreissena rostriformis'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_organism['Organism'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9537322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_organism[data_with_organism['Organism']=='Ciona intestinalis']['length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c63fe60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism')\n",
    "\n",
    "for organism in data_with_organism['Organism'].unique():\n",
    "    \n",
    "    organism_under = '_'.join(organism.split(' '))\n",
    "    \n",
    "    if 'Xenia' in organism:\n",
    "        \n",
    "        organism_under = 'Xenia_sp'\n",
    "        \n",
    "    os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism/'+organism_under)\n",
    "\n",
    "    write_concurrence_func(data_with_organism[data_with_organism['Organism']==organism],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism/'+organism_under)\n",
    "\n",
    "    write_stats(data_with_organism[data_with_organism['Organism']==organism],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism/'+organism_under+'/domain_summary_statistics.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61025df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122328, 37)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_organism.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65f95b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_organism2 = data_with_organism.merge(acceptable_compactors)\n",
    "\n",
    "os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism_contaminant_filtered')\n",
    "\n",
    "for organism in data_with_organism2['Organism'].unique():\n",
    "    \n",
    "    organism_under = '_'.join(organism.split(' '))\n",
    "    \n",
    "    if 'Xenia' in organism:\n",
    "        \n",
    "        organism_under = 'Xenia_sp'\n",
    "        \n",
    "    os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism_contaminant_filtered/'+organism_under)\n",
    "\n",
    "    write_concurrence_func(data_with_organism2[data_with_organism2['Organism']==organism],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism_contaminant_filtered/'+organism_under)\n",
    "\n",
    "    write_stats(data_with_organism2[data_with_organism2['Organism']==organism],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_organism_contaminant_filtered/'+organism_under+'/domain_summary_statistics.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "948619d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_technology2 = data_with_technology.merge(acceptable_compactors)\n",
    "\n",
    "os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered')\n",
    "os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/10x')\n",
    "os.mkdir('/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/SS2')\n",
    "\n",
    "write_stats(data_with_technology2[data_with_technology2['technology']=='SS2'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/SS2/domain_summary_statistics.tsv')\n",
    "write_stats(data_with_technology2[data_with_technology2['technology']=='10x'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/10x/domain_summary_statistics.tsv')\n",
    "\n",
    "write_concurrence_func(data_with_technology2[data_with_technology2['technology']=='SS2'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/SS2')\n",
    "write_concurrence_func(data_with_technology2[data_with_technology2['technology']=='10x'],'/oak/stanford/groups/horence/george/protein_domain_project/workflow/split_by_technology_contaminant_filtered/10x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0926b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pws_acc = pfam_with_stats.merge(acceptable_compactors)\n",
    "\n",
    "write_stats(pws_acc,'/oak/stanford/groups/horence/george/protein_domain_project/workflow/contaminant_filtered_domain_summary_statistics.tsv')\n",
    "write_concurrence_func(pws_acc,'/oak/stanford/groups/horence/george/protein_domain_project')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
